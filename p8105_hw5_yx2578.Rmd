---
title: "Homework 5"
author: Vera Xu
output: github_document
---

This is my solution to HW5.

```{r, message = FALSE, warning = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
  )

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
  )

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1 demo by Jeff in class

Read in the data.

```{r, message = FALSE, warning = FALSE}
homicide_df = 
  read_csv("data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```

Let's look at this a bit

```{r, message = FALSE, warning = FALSE}
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

Can I do a prop test for a single city?

```{r, message = FALSE, warning = FALSE}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved), 
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)
  ) %>% 
  broom::tidy()
```

Try to iterate ........

```{r}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```


```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```



## Problem 2

##### Create a dataframe containing all file names for p2

```{r}
p2_list = 
  list.files("./data/p2_data")
```

#####  Iterate over file names and read in data for each subject

```{r, message = FALSE, warning = FALSE}
p2_load = function(file_list) 
  {
  path = str_c("./data/p2_data/", file_list)
  read_csv(path)
  }

p2_data = 
  purrr::map(p2_list, p2_load) %>%
  bind_rows() %>%
  mutate(file_name = p2_list)
```

#####  Tidy the result

```{r, message = FALSE, warning = FALSE}
p2_tidy_data = 
  p2_data %>%
  mutate(file_name = gsub(".csv", "", file_name)) %>%
  separate(file_name, into = c("trt_group", "id"), sep = "_") %>%
  pivot_longer(week_1:week_8, 
               names_to = "week",
               values_to = "value") %>%
  mutate(week = gsub("week_", "", week))

p2_tidy_data
```

#####  Make a spaghetti plot showing observations on each subject over time

```{r}
p2_tidy_data %>%
  ggplot(aes(x=week, y=value, color=id, group = id)) + 
  geom_point() + 
  geom_line() +
  facet_grid(. ~ trt_group)
```

We can tell from the spaghetti plot above that the values of measurement in the control group is in general lower than the experiment group. The values of control group remained flat over the 8 weeks of follow-up, while the values of experiment group steadily increased over the 8 weeks of follow-up.


## Problem 3

##### Define function *one_sample_ttest* to generate sample dataset and perform one ttest

```{r}
n = 30
sigma = 5

one_sample_ttest = function(mu, h_null) {
  
  sample_data = tibble(
    x = rnorm(n=30, mean = mu, sd = 5),
  )
  
  sample_data %>% 
    t.test(mu = h_null) %>%
    broom::tidy()
}
```

##### Define function *one_sample_ttest* to generate a df for repeated 5000 tests for any given normal mu = k

```{r}
repeat_5000_ttest = function(k) {
  
output_ttest = vector("list", 5000)

for (i in 1:5000) {
  output_ttest[[i]] = one_sample_ttest(mu = k, h_null = 0)
}

ttest_results = bind_rows(output_ttest)
}
```

##### Set mu=0 and perform the result

```{r}
result_0 = 
  repeat_5000_ttest(k=0) %>%
  mutate (mu = 0) %>%
  select(mu, estimate:alternative) %>%
  mutate(decision = case_when(p.value < 0.05 ~ "rej", p.value >=0.05 ~ "fail to rej"))

result_0
```

##### Repeat the test above for mu={1,2,3,4,5,6} and generate a result summary dataframe with decision with regards to the null hypothesis

```{r}
result_1to6 = 
  tibble(mu = c(1, 2, 3, 4, 5, 6)) %>% 
  mutate(
    results = map(.x = mu, ~repeat_5000_ttest(k=.x))
    )

result_summ = 
  result_1to6 %>% 
  unnest(c(mu, results)) %>%
  mutate(decision = case_when(p.value < 0.05 ~ "rej", p.value >=0.05 ~ "fail to rej"))

result_summ
```


##### Make a plot showing the proportion of times the null was rejected and value of mu

```{r, message = FALSE, error= FALSE}
result_summ %>%
  group_by(mu) %>%
  summarize(
    n = n(),
    rej_rate = sum(p.value < 0.05)/n) %>% 
  ggplot(aes(x = mu, y = rej_rate)) +
  geom_point() +
    labs(x = "true mean",
         y = "rejection rate") + 
  xlim(0, 6) + 
  ylim(0, 1)
```

Based on the plot above, as effect size (the difference between true mean and null value) increases, the proportion of times the null was rejected also increases, which means the power of the test increases.

##### Plot describing sample mean and true mean

```{r, message = FALSE, error= FALSE}
library("patchwork")

p1 <-  
  result_summ %>%
  group_by(mu) %>%
  summarize(
    avg_mu_hat = mean(estimate)) %>% 
  ggplot(aes(x = mu, avg_mu_hat)) +
  geom_point() +
    labs(x = "true mean",
         y = "average of estimate",
         title = "All samples") +
  xlim(0, 6) + 
  ylim(0, 6)

p2 <-  
  result_summ %>%
  group_by(mu) %>%
  filter(decision == "rej") %>%
  summarize(
    avg_mu_hat = mean(estimate)) %>% 
  ggplot(aes(x = mu, avg_mu_hat)) +
  geom_point() +
  labs(x = "true mean",
       y = "average of estimate",
       title = "Samples with rejection of null hypothesis")+
  xlim(0, 6) + 
  ylim(0, 6)

p1 + p2
```

In the left plot, where all samples were included in the plot, we can tell that the average estimate is the same as the true mean. This holds true according to the CTL. In the right plot, however, the average of estimate is quite different from the true mean when the power is relatively low (also cross comparing to the power plot). As power increases, the average of estimate gets closer to the true mean among the samples where the null hypothesis is rejected.